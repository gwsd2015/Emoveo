#pseudo code


#take the input file from the user

#file slurp and extract the text


#create regex in order to search subsections of that text
#I think I want to define subsections as /\w{1,50}\n/ or similar pattern
#in order to get that to work I might have to add the roman numberals
#and the numbered system.

#I want the feature to have the subsections and have the user decide
#if he wants to just completely remove the subsection

#the removal would just be removal of text between selected subsection
#and next subsection

#feature to add a 0.20 summary of the subsection before removal would
#be important

#nextly, in order to remove sensitive material, I think that it would make
#more sense to reduce the overall space I have to search so 
#Text::Summarize::En would work best in this part

#in order to make sure that reduction works without removing the
#subsections, I'll just split and summarize the text according to the
#subsections and have it as a finite loop

#the reduction size should probably be 0.45 - 0.75
#this could be a feature available to the user
#simple method of adding <STDIN> and taking the user's input for a valid
#value.

#instead of the wordnet or query, I think I have found a more efficient 
#method. firstly, would be to give the user a document search as a feature
#then generate the amount of related words found. how to find related words
#is to first use the soundex algorithm and then use the levishtein 
#algorithm. this should generate enough positives and false positives.

#then, the user should be able to go through each of the words and the
#20 words before and after it should appear as a string to the user.
#the user would then be able to choose to alter the string 20-word-20
#or block a portion of that word. to chose to block the portion, I think
#would require the subroutine to only grep and read the string set in 
#question. 

#this would repeat until the user is done going through the entire document

#since we are going for a way to make sure that the user has modified the
#document successfully, the application of a tagger is necessary as well
#to find the named entities in the document

#I have been unable to get the Stanford NER to work with Perl and the
#perl once to work on windows so I'm going to use Lingua::EN::Tagger
#and use regex of tags in order to aim for named entities

#I have put this to the test but it seems difficult to remove the tags
#afterwards and get the text back to what it was. but this should be 
#useful to find time and noun patterns. this should allow me to detect
#certain words that even NER can't identify that might be a part of 
#the document to remove so having that is good. 

#I think the time patterns can be manually written as regex

#Regarding named entities, the person is going to have to change the 
#sting set involving the named entities so that meaning can be preserved.

#Grammar rules I think can be defined through Parse::RecDescent so that
#might help in the automatic detection. combine this with the tagger and
#we might have a strong system.

#overall, I want this to be more of a automatic process than user 
#influenced so I will automate any process that I can.

#Lingua::EN::Fathom can be used to identify the readability of the work
#generated by the automatic declassification

#I can also use Lingua::Identify to search for "non-English" words which
#might hint at important information to remove before declassification 
